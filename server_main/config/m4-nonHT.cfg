# Copyright (c) 2013-2021 Contributors to the Eclipse Foundation
# 
# See the NOTICE file(s) distributed with this work for additional
# information regarding copyright ownership.
# 
# This program and the accompanying materials are made available under the
# terms of the Eclipse Public License 2.0 which is available at
# http://www.eclipse.org/legal/epl-2.0
# 
# SPDX-License-Identifier: EPL-2.0
#

#####################################################################################################################
# Appliance profile settings
#####################################################################################################################
# ----------------------------------------------------------
# Number of I/O processor threads (For optimal performance we MUST have I/O listener and NIC IRQs on same CPU socket as I/O Processor threads)
TcpThreads = 5

# ----------------------------------------------------------
# CPU affinity configuration
# Note: thread names with 'SB' (i.e. StandBy) are only active on the StandBy node and are terminated
# when the node switches to Primary mode. haSBCh.HP.* ('HP' = high perf) are critical threads on 
# the standby node; they receive state data from critical thread on the primary node 
# (e.g. tcpiop.x on primary)
### CPU socket 0 ###########################################
# CPU 0 (I/O processor: I/O listener->transport->protocol->engine->protocol->transport->NIC)
Affinity.tcpiop.0=0x1
Affinity.haSBCh.HP.1=0x1
# CPU 1 (I/O processor: I/O listener->transport->protocol->engine->protocol->transport->NIC)
Affinity.tcpiop.1=0x2
Affinity.haSBCh.HP.2=0x2
# CPU 2 (I/O processor: I/O listener->transport->protocol->engine->protocol->transport->NIC)
Affinity.tcpiop.2=0x4
Affinity.haSBCh.HP.3=0x4
# CPU 3 (I/O processor: I/O listener->transport->protocol->engine->protocol->transport->NIC)
Affinity.tcpiop.3=0x8
Affinity.haSBCh.HP.4=0x8
# CPU 4 (I/O processor: I/O listener->transport->protocol->engine->protocol->transport->NIC)
Affinity.tcpiop.4=0x10
Affinity.haSBCh.HP.5=0x10
# CPU 5 (I/O listener tightly coupled with I/O processors therefore MUST be on same CPU socket
# as I/O processors (i.e. epoll_wait thread))
Affinity.tcplisten=0x20
Affinity.haSBCh.HP.6=0x20
# CPU 6 and CPU 7 (irqbalance is not used in version 1.2.  Interrupts are set to CPUS via script:  setirqs.sh
# all non-local IRQs are pinned to CPUs 6 and 7.
# Note: since the Primary<->StandBy node communications is RDMA based there are not a significant
# number of HW interrupts on the StandBy node and thus CPU 6 and 7 can be used on a StandBy node for
# non-IRQ threads
Affinity.haSBAdminCh=0x40

### CPU socket 1 ###########################################
# CPU 8 and CPU 9 MQ Connectivity client
#   see bedrock process descriptor file for MQ Connectivity process 
#   (server_becrock/applications/mqconnectivity/config/processDescriptor.xml using taskset)
# haSBCh.LP.x threads (where x is 1..3) are low perf ('LP') HA channels and are not as critical
Affinity.haSBCh.LP.1=0x100
Affinity.haSBCh.LP.2=0x100
Affinity.haSBCh.LP.3=0x200

# CPU 10 flow control thread (suspended - slow consumers, durable subscribers)
Affinity.tcpdelivery=0x400
Affinity.haSBCh.LP.4=0x400

# CPU 11 (E.K. - 9/29/2012 spoke with I.Z. and these threads pass jobs one to the 
# other: store thread and diskUtilsThread)
Affinity.store=0x800
Affinity.diskUtil=0x800
# haDiskUtil thread (Primary and Standby) is designed to perform lower priority disk task
# and give priority to diskUtil thread (reads generations from disk to memory) during recovery
Affinity.haDiskUtil=0x800
# haControl thread (Primary and Standby) does not consume a lot of CPU cycles, but must not be starved do to its role in heartbeating,
# which is why we give
Affinity.haControl=0x800

# CPU 12 (Connection listener, i.e. accept() thread)
Affinity.tcpconnect=0x1000
Affinity.haSBStoreCh=0x1000

# CPU 13 (timer1 thread, LOW PRIORITY timer thread)
# timer1 timer tasks
#   - MQTT retry (pub expire) task - every 5 seconds wake and check if republish is required
#   - MQTT disconnect task - every 10 seconds wake to check if MQTT connection is still active
#   - transport connection cleanup task - every 3 seconds wake and check to cleanup connection objects
#   - transport receive socket buffer resize task - every 30 seconds increase recv socket buffer if full up to max size
#   - init TLS task - one time init task
#   - cleanup security cache task - every 300 seconds wake and cleanup security cache
#   - engine cleanup task - one time cleanup at termination
#   - update server timestamp task - every 30 seconds (default) wake and check/set server timestamp
#
Affinity.timer1=0x2000
# msgReaper thread performs periodic removal of expired messages on subscriptions and queues, and expired (retained) messages on topics
Affinity.msgReaper=0x2000
Affinity.haSBSyncCh=0x2000

# CPU 14 and 15: (shared with OpenLDAP server (14 and 15) - see server_becrock/applications/openldap/config/processDescriptor.xml)
# ISM Logger (calls syslog-ng APIs) thread
Affinity.logger=0xC000
# ISM Monitoring thread
Affinity.monitoring.1=0xC000
Affinity.monitoring.2=0xC000
# ISM Disconnected Client Notification Thread
Affinity.DncClntNft=0xC000
# timer0 (HIGH PRIORITY TIMER THREAD) - short lived tasks
# timer0 timer tasks
#   - engine memory monitor task - every 500 millis read /proc/meminfo
#   - init TLS task - one time at init time
#   - client ping/reply task - every time there is a JMS client id steal create a one time timer task to ping the client
Affinity.timer0=0xC000
# Locale change detector thread
Affinity.inotify=0xC000
# ismserver (the main thread of the ismserver program) is always last CPU on the system
Affinity.imaserver=0xC000
# AdminHA thread (Primary and Standby) is busy only during new node synchronization and 
# when configuration changes are made through the cli or webui
Affinity.AdminHA=0xC000
Affinity.IMAConfigHA=0xC000
# haSyncCh is active when the node changes to Primary
Affinity.haSyncCh=0xC000

# Admin server Threads
Affinity.AdminIOP=0xC100

# The thread pool size for LDAP.  The thread names are: Security.x 
SecurityThreadPoolSize = 16

# Security threads send synchronous LDAP requests to local(CPU 14 and 15 also) (or external LDAP).
# split half on CPU 14 and half on CPU 15
Affinity.Security.1=0x4000
Affinity.Security.2=0x4000
Affinity.Security.3=0x4000
Affinity.Security.4=0x4000
Affinity.Security.5=0x4000
Affinity.Security.6=0x4000
Affinity.Security.7=0x4000
Affinity.Security.8=0x4000
Affinity.Security.9=0x8000
Affinity.Security.10=0x8000
Affinity.Security.11=0x8000
Affinity.Security.12=0x8000
Affinity.Security.13=0x8000
Affinity.Security.14=0x8000
Affinity.Security.15=0x8000
Affinity.Security.16=0x8000

#####################################################################################################################
# HW specific Store tuning settings
#####################################################################################################################
Store.TotalMemSizeMB=16384
Store.RecoveryMemSizeMB=10240
Store.TransactionRsrvOps=100000

#####################################################################################################################
# HW specific Transport tuning settings
#####################################################################################################################
# Size of buffer pool used by each IOProc thread is (TcpMaxTransportPoolSizeMB * 1024 * 1024)/(TcpNumOfIOProcThreads)
TcpMaxTransportPoolSizeMB = 10240
# Controls degree of per connection message batching versus low latency messaging (-X means sleep X microseconds; +X means perform X number of sched_yields)
# A value of -1 is better for throughput and and a value of 1 is better for low latency
# Also for low end hardware it is better to avoid a positive value
TcpIOPThreadDelayMicro = -1 
# SslUseBuffersPool size is expressed in units of 1K connections, thus a value of 1000 is scaled for 1M connections
SslUseBuffersPool = 1024

#####################################################################################################################
# HW specific Engine tuning settings
#####################################################################################################################
# Limit of the number of unacked MQTT messages allowed per client (this has implication on memory allocation)
# 02/04/13 - Changed from 1000 to 100 based on Thruput testing performed, and impact on memory footprint.  There 
# was a small diff (5%) between 1000 and 100, which was acceptable.
# 03/11/13 - Modified to 128 to be a power of 2.
mqttMsgIdRange=128

